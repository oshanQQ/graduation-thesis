{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39cb558-4651-45ef-b52a-46b1b75c3125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 07:28:18.763502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:18.782583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:18.782798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:18.783833: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 07:28:18.784874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:18.785072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:18.785190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:19.172239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:19.172421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:19.172523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-12 07:28:19.172633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2023-02-12 07:28:29.316875: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 179.44MiB (rounded to 188160000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-02-12 07:28:29.317000: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2023-02-12 07:28:29.317030: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 10, Chunks in use: 10. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 84B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317050: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317069: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317086: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317102: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317119: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317138: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 20.0KiB allocated for chunks. 20.0KiB in use in bin. 20.0KiB client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317155: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 39.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317171: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317187: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317203: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317218: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317236: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 1. 1.53MiB allocated for chunks. 1.53MiB in use in bin. 1.53MiB client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317252: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 0. 3.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317268: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317283: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317301: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 0. 29.40MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317331: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317346: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317362: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317378: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-02-12 07:28:29.317400: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 179.44MiB was 128.00MiB, Chunk State: \n",
      "2023-02-12 07:28:29.317414: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 35651584\n",
      "2023-02-12 07:28:29.317440: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000000 of size 256 next 1\n",
      "2023-02-12 07:28:29.317454: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000100 of size 1280 next 2\n",
      "2023-02-12 07:28:29.317467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000600 of size 256 next 3\n",
      "2023-02-12 07:28:29.317480: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000700 of size 256 next 4\n",
      "2023-02-12 07:28:29.317493: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000800 of size 256 next 5\n",
      "2023-02-12 07:28:29.317508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4000900 of size 2048 next 6\n",
      "2023-02-12 07:28:29.317521: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001100 of size 256 next 9\n",
      "2023-02-12 07:28:29.317534: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001200 of size 256 next 10\n",
      "2023-02-12 07:28:29.317547: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001300 of size 256 next 11\n",
      "2023-02-12 07:28:29.317560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001400 of size 256 next 12\n",
      "2023-02-12 07:28:29.317572: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001500 of size 256 next 15\n",
      "2023-02-12 07:28:29.317585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4001600 of size 256 next 16\n",
      "2023-02-12 07:28:29.317598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84d4001700 of size 40192 next 13\n",
      "2023-02-12 07:28:29.317613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d400b400 of size 20480 next 14\n",
      "2023-02-12 07:28:29.317626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84d4010400 of size 3147008 next 7\n",
      "2023-02-12 07:28:29.317640: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84d4310900 of size 1605632 next 8\n",
      "2023-02-12 07:28:29.317654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84d4498900 of size 30832384 next 18446744073709551615\n",
      "2023-02-12 07:28:29.317667: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2023-02-12 07:28:29.317686: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 10 Chunks of size 256 totalling 2.5KiB\n",
      "2023-02-12 07:28:29.317702: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-02-12 07:28:29.317716: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2023-02-12 07:28:29.317730: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 20480 totalling 20.0KiB\n",
      "2023-02-12 07:28:29.317749: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1605632 totalling 1.53MiB\n",
      "2023-02-12 07:28:29.317764: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 1.56MiB\n",
      "2023-02-12 07:28:29.317778: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 35651584 memory_limit_: 35651584 available bytes: 0 curr_region_allocation_bytes_: 71303168\n",
      "2023-02-12 07:28:29.317806: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                        35651584\n",
      "InUse:                         1632000\n",
      "MaxInUse:                      4818944\n",
      "NumAllocs:                          20\n",
      "MaxAllocSize:                  1605632\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-02-12 07:28:29.317827: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *________*****______________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m801\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m41\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mClassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mClassify\u001b[0;34m(batch_size, epochs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     67\u001b[0m callback \u001b[38;5;241m=\u001b[39m CustomCallback()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     71\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.training_time = None\n",
    "        self.average_memory_utilization = None\n",
    "        self.average_gpu_utilization = None\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.start_time = time.time()\n",
    "        self.memory_utilizations = []\n",
    "        self.gpu_utilizations = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        self.training_time = time.time() - self.start_time\n",
    "        self.average_memory_utilization = np.mean(self.memory_utilizations)\n",
    "        self.average_gpu_utilization = np.mean(self.gpu_utilizations)\n",
    "        print('\\nTotal training time: {} seconds'.format(self.training_time))\n",
    "        print('\\nAverage memory utilization during training so far: {}'.format(self.average_memory_utilization))\n",
    "        print('\\nAverage GPU utilization during training so far: {}'.format(self.average_gpu_utilization))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        memory_utilization = psutil.cpu_percent()\n",
    "        gpu_utilization_process = subprocess.Popen(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader'], stdout=subprocess.PIPE)\n",
    "        gpu_utilization = int(gpu_utilization_process.stdout.readline().strip().decode('utf-8').split()[0])\n",
    "        self.memory_utilizations.append(memory_utilization)\n",
    "        self.gpu_utilizations.append(gpu_utilization)\n",
    "\n",
    "        \n",
    "def Classify(batch_size, epochs):\n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    callback = CustomCallback()\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    \n",
    "    with open('mnist_classification.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([batch_size, epochs, callback.average_memory_utilization, callback.average_gpu_utilization, callback.training_time, test_loss, test_acc])\n",
    "\n",
    "        \n",
    "for batch in range(100, 801, 10):\n",
    "    for epoch in range(10, 41, 10):\n",
    "        Classify(batch, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07011bc5-ce8a-426c-a4a7-768e1605b5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
